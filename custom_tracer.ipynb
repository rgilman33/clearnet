{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/anaconda3/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from model_tracer import *\n",
    "from coords_marker import *\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "directory_path = 'model_specs'\n",
    "shutil.rmtree(directory_path) \n",
    "os.makedirs(directory_path)\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "with open(\"model_specs/model_specs_overview.json\", 'w') as file:\n",
    "    json.dump({}, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model_names = [\n",
    "    'bat_resnext26ts',\n",
    " 'beit_base_patch16_224',\n",
    " 'beitv2_base_patch16_224',\n",
    " 'botnet26t_256',\n",
    " 'caformer_s18',\n",
    " 'cait_s24_224',\n",
    " 'coat_lite_small',\n",
    " 'coat_mini',\n",
    " 'coatnet_0_224',\n",
    " 'convformer_s18',\n",
    " 'convit_small',\n",
    " 'convmixer_768_32',\n",
    " 'convnext_base',\n",
    " 'convnextv2_base',\n",
    " 'crossvit_base_240',\n",
    " 'cs3darknet_focus_s',\n",
    " 'cs3darknet_s',\n",
    " 'cs3se_edgenet_x',\n",
    " 'cs3sedarknet_l',\n",
    " 'cspdarknet53',\n",
    " 'darknet17',\n",
    " 'davit_base',\n",
    " 'deit3_base_patch16_224',\n",
    " 'deit_base_distilled_patch16_224',\n",
    " 'densenet121',\n",
    " 'dla34',\n",
    " 'dm_nfnet_f0',\n",
    " 'dpn48b',\n",
    " 'eca_botnext26ts_256',\n",
    " 'eca_halonext26ts',\n",
    " 'eca_nfnet_l0',\n",
    " 'eca_resnet33ts',\n",
    " 'eca_resnext26ts',\n",
    " 'eca_vovnet39b',\n",
    " 'ecaresnet26t',\n",
    " 'edgenext_base',\n",
    " 'efficientformer_l1',\n",
    " 'efficientformerv2_l',\n",
    " 'efficientnet_b0',\n",
    " 'efficientnetv2_s',\n",
    " 'efficientvit_b0',\n",
    " 'ese_vovnet19b_dw',\n",
    " 'eva02_base_patch14_224',\n",
    " 'eva02_base_patch16_clip_224',\n",
    " 'fastvit_sa12',\n",
    " 'fastvit_t8',\n",
    " 'fbnetc_100',\n",
    " 'fbnetv3_b',\n",
    " 'flexivit_base',\n",
    " 'focalnet_base_lrf',\n",
    " 'focalnet_base_srf',\n",
    " 'gcresnet33ts',\n",
    " 'gcvit_base',\n",
    " 'gernet_s',\n",
    " 'ghostnet_050',\n",
    " 'gmixer_12_224',\n",
    " 'gmlp_b16_224',\n",
    " 'halo2botnet50ts_256',\n",
    " 'halonet26t',\n",
    " 'haloregnetz_b',\n",
    " 'hardcorenas_a',\n",
    " 'hrnet_w18',\n",
    " 'inception_next_base',\n",
    " 'inception_v3',\n",
    " 'inception_v4',\n",
    " 'lambda_resnet26t',\n",
    " 'lamhalobotnet50ts_256',\n",
    " 'lcnet_035',\n",
    " 'legacy_senet154',\n",
    " 'legacy_seresnet18',\n",
    " 'legacy_xception',\n",
    " 'levit_128',\n",
    " 'levit_conv_128',\n",
    " 'maxxvitv2_rmlp_base_rw_224',\n",
    " 'mixer_b16_224',\n",
    " 'mixnet_m',\n",
    " 'mnasnet_050',\n",
    " 'mobilenetv2_035',\n",
    " 'mobilenetv3_rw',\n",
    " 'mobileone_s0',\n",
    " 'mobilevit_s',\n",
    " 'mobilevitv2_050',\n",
    " 'mvitv2_base',\n",
    " 'nasnetalarge',\n",
    " 'nest_base',\n",
    " 'nf_ecaresnet26',\n",
    " 'nf_regnet_b0',\n",
    " 'nf_resnet26',\n",
    " 'nfnet_f0',\n",
    " 'pit_b_224',\n",
    " 'pnasnet5large',\n",
    " 'poolformer_m36',\n",
    " 'poolformerv2_m36',\n",
    " 'pvt_v2_b0',\n",
    " 'regnetv_040',\n",
    " 'repghostnet_050',\n",
    " 'repvgg_a0',\n",
    " 'repvit_m1',\n",
    " 'res2net50_14w_8s',\n",
    " 'resmlp_12_224',\n",
    " 'resnest14d',\n",
    " 'resnet18',\n",
    " 'resnet50',\n",
    " 'resnetv2_50',\n",
    " 'resnext26ts',\n",
    " 'rexnetr_100',\n",
    " 'samvit_base_patch16_224',\n",
    " 'sebotnet33ts_256',\n",
    " 'sedarknet21',\n",
    " 'sehalonet33ts',\n",
    " 'selecsls42',\n",
    " 'semnasnet_050',\n",
    " 'senet154',\n",
    " 'sequencer2d_s',\n",
    " 'seresnet18',\n",
    " 'seresnext26d_32x4d',\n",
    " 'skresnet18',\n",
    " 'spnasnet_100',\n",
    " 'swin_base_patch4_window7_224',\n",
    " 'swin_s3_base_224',\n",
    " 'swinv2_cr_base_224',\n",
    " 'tiny_vit_5m_224',\n",
    " 'tinynet_a',\n",
    " 'tnt_b_patch16_224',\n",
    " 'tresnet_m',\n",
    " 'twins_pcpvt_base',\n",
    " 'twins_svt_base',\n",
    " 'vgg11',\n",
    " 'vgg19',\n",
    " 'visformer_small',\n",
    " 'vit_base_patch8_224',\n",
    " 'vit_base_patch14_dinov2',\n",
    " 'vit_relpos_base_patch16_224',\n",
    " 'volo_d1_224',\n",
    " 'volo_d2_224',\n",
    " 'vovnet39a',\n",
    " 'xception41',\n",
    " 'xcit_medium_24_p8_224',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_model_names = [\n",
    "                    \"efficientnet_b0\",\n",
    "                    \"coat_mini\",\n",
    "                    \"crossvit_base_240\",\n",
    "                    \"densenet121\",\n",
    "                    \"dpn48b\",\n",
    "                    'dla34',\n",
    "                    'lambda_resnet26t',\n",
    "                    'nasnetalarge',\n",
    "                    'res2net50_14w_8s',\n",
    "                    'vovnet39a',\n",
    "                    \"fastvit_s12\",\n",
    "                    \"focalnet_base_lrf\",\n",
    "                    \"gcvit_base\",\n",
    "                    \"hrnet_w18\",\n",
    "                    \"inception_v4\",\n",
    "                    \"tnt_b_patch16_224\"\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " efficientnet_b0\n",
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 751 nodes and 91 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00000841\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 25 extension lines and 25 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.091\n",
      "get output children: 0.01\n",
      "mark dist from end: 0.001\n",
      "mark dist from start: 0.012\n",
      "denote module children: 0.015\n",
      "nestify first stuff: 0.003\n",
      "x pos: 0.004\n",
      "y pos: 2.575\n",
      "marking coords: 2.815\n",
      "\n",
      "\n",
      " coat_mini\n",
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 3635 nodes and 123 modules\n",
      "n global output nodes 3\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00003727\n",
      "no dns for  mod_out 00003757\n",
      "no dns for  fn_out 00001009\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 294 extension lines and 294 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 2.46\n",
      "get output children: 0.162\n",
      "mark dist from end: 0.003\n",
      "mark dist from start: 0.149\n",
      "denote module children: 0.078\n",
      "nestify first stuff: 0.032\n",
      "x pos: 0.18\n",
      "y pos: 4.39\n",
      "marking coords: 8.363\n",
      "\n",
      "\n",
      " crossvit_base_240\n",
      "input shape:  torch.Size([1, 3, 240, 240])\n",
      "there are 1288 nodes and 81 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00001368\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 55 extension lines and 55 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.314\n",
      "get output children: 0.028\n",
      "mark dist from end: 0.001\n",
      "mark dist from start: 0.017\n",
      "denote module children: 0.018\n",
      "nestify first stuff: 0.008\n",
      "x pos: 0.026\n",
      "y pos: 2.359\n",
      "marking coords: 2.907\n",
      "\n",
      "\n",
      " densenet121\n",
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 1908 nodes and 189 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00002096\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 58 extension lines and 535 extension nodes\n",
      "marking 4600 nodes as conditioning\n",
      "making nodes lookup: 0.609\n",
      "get output children: 0.043\n",
      "mark dist from end: 0.002\n",
      "mark dist from start: 0.048\n",
      "denote module children: 0.084\n",
      "nestify first stuff: 0.011\n",
      "x pos: 0.024\n",
      "y pos: 21.764\n",
      "marking coords: 23.056\n",
      "\n",
      "\n",
      " dpn48b\n",
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 903 nodes and 127 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00001029\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 32 extension lines and 32 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.139\n",
      "get output children: 0.007\n",
      "mark dist from end: 0.001\n",
      "mark dist from start: 0.008\n",
      "denote module children: 0.022\n",
      "nestify first stuff: 0.006\n",
      "x pos: 0.012\n",
      "y pos: 3.671\n",
      "marking coords: 3.959\n",
      "\n",
      "\n",
      " dla34\n",
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 396 nodes and 35 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00000432\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 27 extension lines and 27 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.026\n",
      "get output children: 0.001\n",
      "mark dist from end: 0.0\n",
      "mark dist from start: 0.002\n",
      "denote module children: 0.003\n",
      "nestify first stuff: 0.001\n",
      "x pos: 0.002\n",
      "y pos: 1.074\n",
      "marking coords: 1.146\n",
      "\n",
      "\n",
      " lambda_resnet26t\n",
      "input shape:  torch.Size([1, 3, 256, 256])\n",
      "there are 581 nodes and 79 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00000659\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 20 extension lines and 20 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.066\n",
      "get output children: 0.003\n",
      "mark dist from end: 0.0\n",
      "mark dist from start: 0.003\n",
      "denote module children: 0.009\n",
      "nestify first stuff: 0.003\n",
      "x pos: 0.007\n",
      "y pos: 2.394\n",
      "marking coords: 2.585\n",
      "\n",
      "\n",
      " nasnetalarge\n",
      "input shape:  torch.Size([1, 3, 331, 331])\n",
      "there are 3846 nodes and 431 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00004276\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 85 extension lines and 106 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 2.821\n",
      "get output children: 0.154\n",
      "mark dist from end: 0.003\n",
      "mark dist from start: 0.175\n",
      "denote module children: 0.459\n",
      "nestify first stuff: 0.013\n",
      "x pos: 0.044\n",
      "y pos: 13.373\n",
      "marking coords: 17.437\n",
      "\n",
      "\n",
      " res2net50_14w_8s\n",
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 1332 nodes and 26 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00001357\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 176 extension lines and 176 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.325\n",
      "get output children: 0.029\n",
      "mark dist from end: 0.001\n",
      "mark dist from start: 0.028\n",
      "denote module children: 0.008\n",
      "nestify first stuff: 0.011\n",
      "x pos: 0.056\n",
      "y pos: 1.3\n",
      "marking coords: 1.904\n",
      "\n",
      "\n",
      " vovnet39a\n",
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 575 nodes and 103 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00000677\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 30 extension lines and 30 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.056\n",
      "get output children: 0.003\n",
      "mark dist from end: 0.0\n",
      "mark dist from start: 0.003\n",
      "denote module children: 0.01\n",
      "nestify first stuff: 0.002\n",
      "x pos: 0.004\n",
      "y pos: 3.395\n",
      "marking coords: 3.578\n",
      "\n",
      "\n",
      " fastvit_s12\n",
      "input shape:  torch.Size([1, 3, 256, 256])\n",
      "there are 1295 nodes and 217 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00001511\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 44 extension lines and 44 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.288\n",
      "get output children: 0.015\n",
      "mark dist from end: 0.002\n",
      "mark dist from start: 0.028\n",
      "denote module children: 0.06\n",
      "nestify first stuff: 0.005\n",
      "x pos: 0.007\n",
      "y pos: 6.849\n",
      "marking coords: 8.04\n",
      "\n",
      "\n",
      " focalnet_base_lrf\n",
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 2731 nodes and 255 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00002985\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 168 extension lines and 168 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 1.374\n",
      "get output children: 0.083\n",
      "mark dist from end: 0.002\n",
      "mark dist from start: 0.097\n",
      "denote module children: 0.135\n",
      "nestify first stuff: 0.016\n",
      "x pos: 0.063\n",
      "y pos: 8.125\n",
      "marking coords: 10.355\n",
      "\n",
      "\n",
      " gcvit_base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/anaconda3/envs/py311/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449183005/work/aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 3433 nodes and 237 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00003703\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 163 extension lines and 170 extension nodes\n",
      "marking 153 nodes as conditioning\n",
      "making nodes lookup: 2.451\n",
      "get output children: 0.139\n",
      "mark dist from end: 0.004\n",
      "mark dist from start: 0.141\n",
      "denote module children: 0.175\n",
      "nestify first stuff: 0.022\n",
      "x pos: 0.056\n",
      "y pos: 9.26\n",
      "marking coords: 13.094\n",
      "\n",
      "\n",
      " hrnet_w18\n",
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 3198 nodes and 278 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00003475\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 172 extension lines and 172 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 1.96\n",
      "get output children: 0.117\n",
      "mark dist from end: 0.003\n",
      "mark dist from start: 0.122\n",
      "denote module children: 0.179\n",
      "nestify first stuff: 0.02\n",
      "x pos: 0.039\n",
      "y pos: 9.179\n",
      "marking coords: 12.01\n",
      "\n",
      "\n",
      " inception_v4\n",
      "input shape:  torch.Size([1, 3, 299, 299])\n",
      "there are 2071 nodes and 361 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00002431\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 9 extension lines and 9 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.823\n",
      "get output children: 0.043\n",
      "mark dist from end: 0.003\n",
      "mark dist from start: 0.049\n",
      "denote module children: 0.179\n",
      "nestify first stuff: 0.009\n",
      "x pos: 0.011\n",
      "y pos: 11.613\n",
      "marking coords: 13.033\n",
      "\n",
      "\n",
      " tnt_b_patch16_224\n",
      "input shape:  torch.Size([1, 3, 224, 224])\n",
      "there are 1698 nodes and 62 modules\n",
      "n global output nodes 2\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00001747\n",
      "no dns for  mod_out 00001759\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 133 extension lines and 133 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.584\n",
      "get output children: 0.043\n",
      "mark dist from end: 0.002\n",
      "mark dist from start: 0.031\n",
      "denote module children: 0.027\n",
      "nestify first stuff: 0.01\n",
      "x pos: 0.031\n",
      "y pos: 2.405\n",
      "marking coords: 3.288\n"
     ]
    }
   ],
   "source": [
    "model_specs_overview = {}\n",
    "failed_models = []\n",
    "# timm_model_names = [\"mvitv2_base\"] \n",
    "for mn in timm_model_names:\n",
    "    print(\"\\n\\n\", mn)\n",
    "\n",
    "    model = timm.create_model(mn, pretrained=False)\n",
    "    model = model.eval()\n",
    "    input_shape = model.default_cfg[\"input_size\"]\n",
    "\n",
    "    try:\n",
    "        with Recorder():\n",
    "            input_data = torch.randn(*input_shape)[None, ...] # pad one for batch\n",
    "            print(\"input shape: \", input_data.shape)    \n",
    "            with torch.no_grad():\n",
    "                output = model(input_data)\n",
    "\n",
    "        model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "        model_coords_marker.compile()\n",
    "\n",
    "    except Exception as e: \n",
    "        failed_models.append((mn, e))\n",
    "        print(\"couldn't do\", mn, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "failed_models # gcvit broke in JS recursive edges when we added tensor nodes, or before maybe when made mods not inputs if have upstream inputs also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm models: RecorderTensor has no edge_id (volos)\n",
    "# dist from end measurement i believe is what takes forever on eg gemma\n",
    "# __setitem__\n",
    "\n",
    "# i think remove unused inputs, but still outputting counts as using\n",
    "    # trying this now. Check against all models before proceeding too far.\n",
    "\n",
    "# trying using execution_counter instead of n_upstream_nodes to sort global outputs. Much faster. Though technically i like prev better.\n",
    "\n",
    "# what about when have multiple tensors, still want to consolidate. Eg stylegan\n",
    "# dist from end calc, we're still using false ends is that true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", token=token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", token=token)\n",
    "\n",
    "input_text = \"A\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>A researcher\n",
      "there are 5231 nodes and 130 modules\n",
      "n global output nodes 39\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 4\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 1356 extension lines and 1426 extension nodes\n",
      "marking 943 nodes as conditioning\n",
      "making nodes lookup: 5.522\n",
      "get output children: 0.306\n",
      "mark dist from end: 0.007\n",
      "mark dist from start: 0.33\n",
      "denote module children: 0.11\n",
      "nestify first stuff: 0.055\n",
      "x pos: 0.497\n",
      "y pos: 35.459\n",
      "marking coords: 42.806\n"
     ]
    }
   ],
   "source": [
    "with Recorder():\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**input_ids, \n",
    "                                #  max_length=input_ids[\"input_ids\"].shape[1],\n",
    "                                 max_new_tokens=1,\n",
    "                                 ) #NOTE had to manually cap output, otherwise huge graph generated\n",
    "        print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "mn = \"gemma-2b\" \n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:  67%|██████▋   | 2/3 [08:09<04:04, 244.81s/it]"
     ]
    }
   ],
   "source": [
    "# paligemma\n",
    "\n",
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "model_id = \"google/paligemma-3b-mix-224\"\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, token=token).eval()\n",
    "processor = AutoProcessor.from_pretrained(model_id, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Instruct the model to create a caption in Spanish\n",
    "prompt = \"caption es\"\n",
    "model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**model_inputs, max_new_tokens=1, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\", \n",
    "    # device_map=\"cuda\", \n",
    "    # torch_dtype=\"auto\", \n",
    "    trust_remote_code=True, \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
    "]\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "generation_args = {\n",
    "    \"max_new_tokens\": 500,\n",
    "    \"return_full_text\": False,\n",
    "    \"temperature\": 0.0,\n",
    "    \"do_sample\": False,\n",
    "}\n",
    "\n",
    "output = pipe(messages, **generation_args)\n",
    "print(output[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# devlog. Needed to add embeddings to non-skipped modules, bc that's the only place where RecorderTensors are made\n",
    "# TODO we need to do better job of making sure we're never missing them. Then we changed how we're measuring parenthood context,\n",
    "# which was a small change codewise but pretty fundamental. This cleared up the CLIP issue where parentage wasn't correct, which i \n",
    "# still don't know what caused it. Then had to fix a bug in SD bc of the extra output nodes we have been appending when have the\n",
    "# same tensor in output multiple times. Wasn't a problem before bc we were doing a two-step thing, so covered up the bug, but\n",
    "# then got exposed when refactored for the parenthood change. Still have multiple issues, but models are coming together. Review\n",
    "# what we've got up in dashboard right now and catelog where we're at. Keep chipping away.\n",
    "\n",
    "# As a review, in Frigiliana we finished up the majority of the layout engine, since being in Ronda two weeks we've been working\n",
    "# on getting more, larger models into the system, which necessitated rewriting our backend. I see now that this won't be polished \n",
    "# before Bologna, but I would like to get all the major stuctural things solid beforehand. All of the major models in. All of the \n",
    "# layout solid, mostly thinking conditioning here, though in the complex models things are sometimes getting messy. \n",
    "# ControlNet still need to add, gigagan needs to debug to get running, and the current models have a lot to be debugged. \n",
    "# It will be mostly debugging swamp for the next two weeks i imagine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "there are 2914 nodes and 136 modules\n",
      "n global output nodes 1\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 3\n",
      "no dns for  mod_out 00003049\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 179 extension lines and 210 extension nodes\n",
      "marking 544 nodes as conditioning\n",
      "making nodes lookup: 1.397\n",
      "get output children: 0.092\n",
      "mark dist from end: 0.004\n",
      "mark dist from start: 0.094\n",
      "denote module children: 0.075\n",
      "nestify first stuff: 0.013\n",
      "x pos: 0.056\n",
      "y pos: 6.566\n",
      "marking coords: 8.67\n"
     ]
    }
   ],
   "source": [
    "# vanilla SD, manual\n",
    "\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "\n",
    "model = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "\n",
    "with Recorder():\n",
    "    input_data = [torch.randn(1,4,64,64), torch.tensor([1]), torch.randn(1, 12, 768)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(*input_data)\n",
    "mn = \"stable-diffusion-v1-4\" \n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# works fine, just superfluous for testing\n",
    "\n",
    "# import torch\n",
    "# from diffusers import StableDiffusionPipeline\n",
    "# from model_tracer import *\n",
    "# from coords_marker import *\n",
    "\n",
    "# pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\")\n",
    "# # pipe.to(\"cuda\")\n",
    "# # pipe.enable_attention_slicing() # this was resulting in __setitem__s, which we don't handle yet. Each one results in global output\n",
    "# # takes 15 min rather than less than two\n",
    "\n",
    "# prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "\n",
    "# with Recorder():\n",
    "#     with torch.no_grad():\n",
    "#         image = pipe(prompt, num_inference_steps=1).images[0] \n",
    "\n",
    "# mn = \"stable-diffusion-v1-4-FULL\"\n",
    "# model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "# model_coords_marker.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 728 nodes and 45 modules\n",
      "n global output nodes 3\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  fn_out 00000346\n",
      "no dns for  fn_out 00000344\n",
      "no dns for  mod_out 00000772\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 27 extension lines and 27 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.113\n",
      "get output children: 0.01\n",
      "mark dist from end: 0.001\n",
      "mark dist from start: 0.006\n",
      "denote module children: 0.01\n",
      "nestify first stuff: 0.004\n",
      "x pos: 0.012\n",
      "y pos: 1.896\n",
      "marking coords: 2.245\n"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "model = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "\n",
    "with Recorder():\n",
    "    input_data = torch.randn(1,3,512,512)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "        \n",
    "mn = \"AutoencoderKL\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/anaconda3/envs/py311/lib/python3.11/site-packages/datasets/load.py:1461: FutureWarning: The repository for huggingface/cats-image contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/huggingface/cats-image\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n",
      "/home/beans/anaconda3/envs/py311/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got duplicate tensors in output bundle\n",
      "there are 1903 nodes and 131 modules\n",
      "n global output nodes 5\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 18\n",
      "no dns for  mod_out 00002043\n",
      "no dns for  mod_out 00002045\n",
      "no dns for  mod_out 00001725\n",
      "no dns for  mod_out 00000725\n",
      "no dns for  mod_out 00000373\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 13. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 74 extension lines and 74 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.707\n",
      "get output children: 0.042\n",
      "mark dist from end: 0.003\n",
      "mark dist from start: 0.041\n",
      "denote module children: 0.052\n",
      "nestify first stuff: 0.008\n",
      "x pos: 0.078\n",
      "y pos: 4.698\n",
      "marking coords: 5.951\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, Swinv2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "model = Swinv2Model.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n",
    "model = model.eval()\n",
    "\n",
    "with Recorder():\n",
    "\n",
    "    inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "mn = \"swinv2-transformers\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: padding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 2233 nodes and 106 modules\n",
      "n global output nodes 8\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 6\n",
      "no dns for  mod_out 00002334\n",
      "no dns for  mod_out 00002328\n",
      "no dns for  mod_out 00002326\n",
      "no dns for  mod_out 00002324\n",
      "no dns for  mod_out 00002332\n",
      "no dns for  mod_out 00002336\n",
      "no dns for  mod_out 00002338\n",
      "no dns for  mod_out 00002330\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 3. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 162 extension lines and 183 extension nodes\n",
      "marking 143 nodes as conditioning\n",
      "making nodes lookup: 0.981\n",
      "get output children: 0.06\n",
      "mark dist from end: 0.002\n",
      "mark dist from start: 0.066\n",
      "denote module children: 0.062\n",
      "nestify first stuff: 0.013\n",
      "x pos: 0.059\n",
      "y pos: 4.413\n",
      "marking coords: 5.936\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "with Recorder():\n",
    "    inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "mn = \"CLIP\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 320x640 (no detections), 101.4ms\n",
      "Speed: 1.1ms preprocess, 101.4ms inference, 3.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "there are 1346 nodes and 179 modules\n",
      "no execution counter for global output node  __setitem__\n",
      "no execution counter for global output node  __setitem__\n",
      "no execution counter for global output node  __setitem__\n",
      "no execution counter for global output node  __setitem__\n",
      "no execution counter for global output node  __setitem__\n",
      "n global output nodes 13\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 2\n",
      "no dns for  mod_out 00000729\n",
      "no dns for  mod_out 00000727\n",
      "no dns for  mod_out 00000731\n",
      "no dns for  mod_out 00000733\n",
      "no dns for  mod_out 00001473\n",
      "no dns for  __setitem__ 00001518\n",
      "no dns for  fn_out 00001524\n",
      "no dns for  __setitem__ 00001502\n",
      "no dns for  __setitem__ 00001507\n",
      "no dns for  __setitem__ 00001512\n",
      "no dns for  __setitem__ 00001517\n",
      "no dns for  mod_out 00001475\n",
      "no dns for  mod_out 00001477\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 2. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 83 extension lines and 83 extension nodes\n",
      "marking 4 nodes as conditioning\n",
      "making nodes lookup: 0.344\n",
      "get output children: 0.017\n",
      "mark dist from end: 0.002\n",
      "mark dist from start: 0.03\n",
      "denote module children: 0.044\n",
      "nestify first stuff: 0.005\n",
      "x pos: 0.02\n",
      "y pos: 4.931\n",
      "marking coords: 5.56\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8n.pt')  # pretrained YOLOv8n model\n",
    "\n",
    "with Recorder():\n",
    "    input_data = torch.randn(1,3,320,640) / 10\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_data)\n",
    "\n",
    "mn = \"yolo\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still not making sure all RecorderTensors are caught, eg Embeddings in CLIP. Need to be more \n",
    "# thorough here. Yolo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is yolo only single pass when on cpu? yes appears so, autobackend is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beans/anaconda3/envs/py311/lib/python3.11/site-packages/datasets/load.py:1461: FutureWarning: The repository for huggingface/cats-image contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/huggingface/cats-image\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1016 nodes and 100 modules\n",
      "n global output nodes 2\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 1\n",
      "no dns for  mod_out 00001125\n",
      "no dns for  mod_out 00001127\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 49 extension lines and 49 extension nodes\n",
      "marking 0 nodes as conditioning\n",
      "making nodes lookup: 0.225\n",
      "get output children: 0.013\n",
      "mark dist from end: 0.003\n",
      "mark dist from start: 0.017\n",
      "denote module children: 0.028\n",
      "nestify first stuff: 0.007\n",
      "x pos: 0.019\n",
      "y pos: 3.579\n",
      "marking coords: 4.037\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, Dinov2Model\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n",
    "model = model.eval()\n",
    "\n",
    "with Recorder():\n",
    "    inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "mn = \"dinov2\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 849 nodes and 51 modules\n",
      "n global output nodes 2\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 3\n",
      "no dns for  mod_out 00000893\n",
      "no dns for  fn_out 00000899\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 1. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 83 extension lines and 88 extension nodes\n",
      "marking 520 nodes as conditioning\n",
      "making nodes lookup: 0.15\n",
      "get output children: 0.008\n",
      "mark dist from end: 0.001\n",
      "mark dist from start: 0.01\n",
      "denote module children: 0.017\n",
      "nestify first stuff: 0.006\n",
      "x pos: 0.02\n",
      "y pos: 2.577\n",
      "marking coords: 2.934\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "from stylegan2_pytorch import ModelLoader\n",
    "\n",
    "loader = ModelLoader(\n",
    "    base_dir = '',\n",
    "    name = 'default'                   # the project name, defaults to 'default'\n",
    ")\n",
    "with Recorder():\n",
    "    noise   = torch.randn(1, 512).cuda() # noise\n",
    "\n",
    "    # styles  = loader.noise_to_styles(noise, trunc_psi = 0.7)  \n",
    "    # pass through mapping network. trunc_psi causes like 100 StyleVector networks to run, resulting in 20k nodes.\n",
    "    # note how this looked: we had 100 of the same network, so the same nn represented over and over, the results\n",
    "    # were catted together then cpu() then numpy() nodes. We need to think through how we want this to look.\n",
    "    styles  = loader.noise_to_styles(noise)  # pass through mapping network\n",
    "    images  = loader.styles_to_images(styles) # call the generator on intermediate style vectors\n",
    "\n",
    "mn = \"stylegan-2\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from gigagan_pytorch import (\n",
    "#     GigaGAN,\n",
    "#     ImageDataset\n",
    "# )\n",
    "\n",
    "# with Recorder():\n",
    "#     gan = GigaGAN(\n",
    "#         train_upsampler = True,     # set this to True\n",
    "#         generator = dict(\n",
    "#             style_network = dict(\n",
    "#                 dim = 64,\n",
    "#                 depth = 4\n",
    "#             ),\n",
    "#             dim = 32,\n",
    "#             image_size = 256,\n",
    "#             input_image_size = 64,\n",
    "#             unconditional = True\n",
    "#         ),\n",
    "#         discriminator = dict(\n",
    "#             dim_capacity = 16,\n",
    "#             dim_max = 512,\n",
    "#             image_size = 256,\n",
    "#             num_skip_layers_excite = 4,\n",
    "#             multiscale_input_resolutions = (128,),\n",
    "#             unconditional = True\n",
    "#         ),\n",
    "#         amp = True\n",
    "#     )#.cuda()\n",
    "\n",
    "#     lowres = torch.randn(1, 3, 64, 64)#.cuda()\n",
    "\n",
    "#     images = gan.generate(lowres) # (1, 3, 256, 256)\n",
    "# images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False).eval()\n",
    "# with Recorder():\n",
    "#     input_data = torch.randn(1,3,256,256) \n",
    "#     with torch.no_grad():\n",
    "#         out = model(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  3.53it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [01:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/beans/clearnet/custom_tracer.ipynb Cell 40\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/beans/clearnet/custom_tracer.ipynb#X55sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwith\u001b[39;00m Recorder():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/beans/clearnet/custom_tracer.ipynb#X55sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma photo of an astronaut riding a horse on mars\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/beans/clearnet/custom_tracer.ipynb#X55sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     image \u001b[39m=\u001b[39m pipe(prompt, num_inference_steps\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39mimages[\u001b[39m0\u001b[39m]  \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beans/clearnet/custom_tracer.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m mn \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msd-1.5-runway\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/beans/clearnet/custom_tracer.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model_coords_marker \u001b[39m=\u001b[39m ModelCoordsMarker(gc\u001b[39m.\u001b[39mnodes, gc\u001b[39m.\u001b[39mmodules, mn)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:971\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[0;34m(self, prompt, height, width, num_inference_steps, timesteps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    968\u001b[0m latent_model_input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mscale_model_input(latent_model_input, t)\n\u001b[1;32m    970\u001b[0m \u001b[39m# predict the noise residual\u001b[39;00m\n\u001b[0;32m--> 971\u001b[0m noise_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49munet(\n\u001b[1;32m    972\u001b[0m     latent_model_input,\n\u001b[1;32m    973\u001b[0m     t,\n\u001b[1;32m    974\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mprompt_embeds,\n\u001b[1;32m    975\u001b[0m     timestep_cond\u001b[39m=\u001b[39;49mtimestep_cond,\n\u001b[1;32m    976\u001b[0m     cross_attention_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_attention_kwargs,\n\u001b[1;32m    977\u001b[0m     added_cond_kwargs\u001b[39m=\u001b[39;49madded_cond_kwargs,\n\u001b[1;32m    978\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    979\u001b[0m )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    981\u001b[0m \u001b[39m# perform guidance\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_classifier_free_guidance:\n",
      "File \u001b[0;32m~/clearnet/model_tracer.py:191\u001b[0m, in \u001b[0;36mRecorder.__enter__.<locals>.new_module_forward\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m kwargs \u001b[39m=\u001b[39m traverse_arbitrary_thing(kwargs, enrich_tensor_arg, [RecorderTensor, torch\u001b[39m.\u001b[39mTensor])\n\u001b[1;32m    190\u001b[0m \u001b[39m# Run normal forward, using our enriched args. Functions and submodules called in here.\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m out \u001b[39m=\u001b[39m orig_module_forward(mod, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[39m# Pad w output nodes off each output\u001b[39;00m\n\u001b[1;32m    194\u001b[0m added_nodes_ids \u001b[39m=\u001b[39m [] \u001b[39m# SD was getting duplicates of same tensor inside output bundle\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_condition.py:1292\u001b[0m, in \u001b[0;36mUNet2DConditionModel.forward\u001b[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         sample \u001b[39m=\u001b[39m upsample_block(\n\u001b[1;32m   1282\u001b[0m             hidden_states\u001b[39m=\u001b[39msample,\n\u001b[1;32m   1283\u001b[0m             temb\u001b[39m=\u001b[39memb,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1289\u001b[0m             encoder_attention_mask\u001b[39m=\u001b[39mencoder_attention_mask,\n\u001b[1;32m   1290\u001b[0m         )\n\u001b[1;32m   1291\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1292\u001b[0m         sample \u001b[39m=\u001b[39m upsample_block(\n\u001b[1;32m   1293\u001b[0m             hidden_states\u001b[39m=\u001b[39;49msample,\n\u001b[1;32m   1294\u001b[0m             temb\u001b[39m=\u001b[39;49memb,\n\u001b[1;32m   1295\u001b[0m             res_hidden_states_tuple\u001b[39m=\u001b[39;49mres_samples,\n\u001b[1;32m   1296\u001b[0m             upsample_size\u001b[39m=\u001b[39;49mupsample_size,\n\u001b[1;32m   1297\u001b[0m         )\n\u001b[1;32m   1299\u001b[0m \u001b[39m# 6. post-process\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_norm_out:\n",
      "File \u001b[0;32m~/clearnet/model_tracer.py:191\u001b[0m, in \u001b[0;36mRecorder.__enter__.<locals>.new_module_forward\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m kwargs \u001b[39m=\u001b[39m traverse_arbitrary_thing(kwargs, enrich_tensor_arg, [RecorderTensor, torch\u001b[39m.\u001b[39mTensor])\n\u001b[1;32m    190\u001b[0m \u001b[39m# Run normal forward, using our enriched args. Functions and submodules called in here.\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m out \u001b[39m=\u001b[39m orig_module_forward(mod, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[39m# Pad w output nodes off each output\u001b[39;00m\n\u001b[1;32m    194\u001b[0m added_nodes_ids \u001b[39m=\u001b[39m [] \u001b[39m# SD was getting duplicates of same tensor inside output bundle\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/diffusers/models/unets/unet_2d_blocks.py:2664\u001b[0m, in \u001b[0;36mUpBlock2D.forward\u001b[0;34m(self, hidden_states, res_hidden_states_tuple, temb, upsample_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2660\u001b[0m             hidden_states \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m   2661\u001b[0m                 create_custom_forward(resnet), hidden_states, temb\n\u001b[1;32m   2662\u001b[0m             )\n\u001b[1;32m   2663\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2664\u001b[0m         hidden_states \u001b[39m=\u001b[39m resnet(hidden_states, temb)\n\u001b[1;32m   2666\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2667\u001b[0m     \u001b[39mfor\u001b[39;00m upsampler \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers:\n",
      "File \u001b[0;32m~/clearnet/model_tracer.py:191\u001b[0m, in \u001b[0;36mRecorder.__enter__.<locals>.new_module_forward\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m kwargs \u001b[39m=\u001b[39m traverse_arbitrary_thing(kwargs, enrich_tensor_arg, [RecorderTensor, torch\u001b[39m.\u001b[39mTensor])\n\u001b[1;32m    190\u001b[0m \u001b[39m# Run normal forward, using our enriched args. Functions and submodules called in here.\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m out \u001b[39m=\u001b[39m orig_module_forward(mod, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    193\u001b[0m \u001b[39m# Pad w output nodes off each output\u001b[39;00m\n\u001b[1;32m    194\u001b[0m added_nodes_ids \u001b[39m=\u001b[39m [] \u001b[39m# SD was getting duplicates of same tensor inside output bundle\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/diffusers/models/resnet.py:346\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb, *args, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m     input_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(input_tensor)\n\u001b[1;32m    344\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(hidden_states)\n\u001b[0;32m--> 346\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(hidden_states)\n\u001b[1;32m    348\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_emb_proj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    349\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskip_time_act:\n",
      "File \u001b[0;32m~/clearnet/model_tracer.py:125\u001b[0m, in \u001b[0;36mRecorder.__enter__.<locals>.new_module_forward\u001b[0;34m(mod, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_module_forward\u001b[39m(mod, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    122\u001b[0m     \u001b[39m# print(\"\\nFORWARD MOD\", mod.__class__.__name__, [type(a) for a in args], kwargs)\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m mod\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39min\u001b[39;00m to_exclude_modules:\n\u001b[0;32m--> 125\u001b[0m         out \u001b[39m=\u001b[39m orig_module_forward(mod, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    126\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    128\u001b[0m     \u001b[39m# print(\"\\n\\nstart \",  mod.__class__.__name__)\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     \u001b[39m# Module node\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "File \u001b[0;32m~/clearnet/model_tracer.py:412\u001b[0m, in \u001b[0;36mRecorderTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m kwargs \u001b[39m=\u001b[39m traverse_arbitrary_thing(kwargs, process_input_tensor, [RecorderTensor])\n\u001b[1;32m    411\u001b[0m \u001b[39m# normal fn. Note we're not modifying the input args, so this could be earlier\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m out \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, kwargs)\n\u001b[1;32m    414\u001b[0m \u001b[39m######################################\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[39m# Add new edge for output tensors, cache info on tensors themselves\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_output\u001b[39m(t):\n",
      "File \u001b[0;32m~/anaconda3/envs/py311/lib/python3.11/site-packages/torch/_tensor.py:1386\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1385\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1386\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1387\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1388\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id)\n",
    "# pipe = pipe.to(\"cuda\")\n",
    "\n",
    "with Recorder():\n",
    "    prompt = \"a photo of an astronaut riding a horse on mars\"\n",
    "    image = pipe(prompt, num_inference_steps=1).images[0]  \n",
    "\n",
    "mn = \"sd-1.5-runway\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:00<00:00,  7.11it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "import torch\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\")\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet)\n",
    "\n",
    "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
    "# pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:41<00:00, 41.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 9015 nodes and 434 modules\n",
      "no execution counter for global output node  numpy\n",
      "no execution counter for global output node  numpy\n",
      "no execution counter for global output node  numpy\n",
      "no execution counter for global output node  numpy\n",
      "n global output nodes 7\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 13\n",
      "no dns for  mod_out 00009407\n",
      "no dns for  numpy 00009420\n",
      "no dns for  numpy 00009429\n",
      "no dns for  numpy 00007346\n",
      "no dns for  numpy 00009448\n",
      "no dns for  mod_out 00001148\n",
      "no dns for  mod_out 00002303\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 5. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 551 extension lines and 610 extension nodes\n",
      "marking 969 nodes as conditioning\n",
      "making nodes lookup: 32.28\n",
      "get output children: 1.223\n",
      "mark dist from end: 0.03\n",
      "mark dist from start: 1.3\n",
      "denote module children: 1.435\n",
      "nestify first stuff: 0.059\n",
      "x pos: 0.238\n",
      "y pos: 36.715\n",
      "marking coords: 74.615\n"
     ]
    }
   ],
   "source": [
    "from diffusers.utils import load_image, make_image_grid\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "original_image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
    ")\n",
    "\n",
    "image = np.array(original_image)\n",
    "\n",
    "low_threshold = 100\n",
    "high_threshold = 200\n",
    "\n",
    "image = cv2.Canny(image, low_threshold, high_threshold)\n",
    "image = image[:, :, None]\n",
    "image = np.concatenate([image, image, image], axis=2)\n",
    "canny_image = Image.fromarray(image)\n",
    "\n",
    "with Recorder():\n",
    "    output = pipe(\n",
    "        \"the mona lisa\", image=canny_image, num_inference_steps=1\n",
    "    ).images[0]\n",
    "\n",
    "mn = \"sd-1.5-runway-ControlNet\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 6/6 [00:04<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:01<00:00, 61.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 11885 nodes and 567 modules\n",
      "n global output nodes 68\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 11\n",
      "no dns for  fn_out 00012451\n",
      "no dns for  mod_out 00003273\n",
      "no dns for  mod_out 00003275\n",
      "no dns for  mod_out 00003277\n",
      "no dns for  mod_out 00003279\n",
      "no dns for  mod_out 00003281\n",
      "no dns for  mod_out 00003283\n",
      "no dns for  mod_out 00003285\n",
      "no dns for  mod_out 00003287\n",
      "no dns for  mod_out 00003289\n",
      "no dns for  mod_out 00003291\n",
      "no dns for  mod_out 00003293\n",
      "no dns for  mod_out 00003295\n",
      "no dns for  mod_out 00003297\n",
      "no dns for  mod_out 00003299\n",
      "no dns for  mod_out 00003301\n",
      "no dns for  mod_out 00003303\n",
      "no dns for  mod_out 00003305\n",
      "no dns for  mod_out 00003307\n",
      "no dns for  mod_out 00003309\n",
      "no dns for  mod_out 00003311\n",
      "no dns for  mod_out 00003313\n",
      "no dns for  mod_out 00003315\n",
      "no dns for  mod_out 00003317\n",
      "no dns for  mod_out 00003319\n",
      "no dns for  mod_out 00003321\n",
      "no dns for  mod_out 00003323\n",
      "no dns for  mod_out 00003325\n",
      "no dns for  mod_out 00003327\n",
      "no dns for  mod_out 00003329\n",
      "no dns for  mod_out 00003331\n",
      "no dns for  mod_out 00003333\n",
      "no dns for  mod_out 00003335\n",
      "no dns for  mod_out 00003271\n",
      "no dns for  mod_out 00006621\n",
      "no dns for  mod_out 00006623\n",
      "no dns for  mod_out 00006625\n",
      "no dns for  mod_out 00006627\n",
      "no dns for  mod_out 00006629\n",
      "no dns for  mod_out 00006631\n",
      "no dns for  mod_out 00006633\n",
      "no dns for  mod_out 00006635\n",
      "no dns for  mod_out 00006637\n",
      "no dns for  mod_out 00006639\n",
      "no dns for  mod_out 00006641\n",
      "no dns for  mod_out 00006643\n",
      "no dns for  mod_out 00006645\n",
      "no dns for  mod_out 00006647\n",
      "no dns for  mod_out 00006649\n",
      "no dns for  mod_out 00006651\n",
      "no dns for  mod_out 00006653\n",
      "no dns for  mod_out 00006655\n",
      "no dns for  mod_out 00006657\n",
      "no dns for  mod_out 00006659\n",
      "no dns for  mod_out 00006661\n",
      "no dns for  mod_out 00006663\n",
      "no dns for  mod_out 00006665\n",
      "no dns for  mod_out 00006667\n",
      "no dns for  mod_out 00006669\n",
      "no dns for  mod_out 00006671\n",
      "no dns for  mod_out 00006673\n",
      "no dns for  mod_out 00006675\n",
      "no dns for  mod_out 00006677\n",
      "no dns for  mod_out 00006679\n",
      "no dns for  mod_out 00006681\n",
      "no dns for  mod_out 00006683\n",
      "no dns for  mod_out 00006619\n",
      "no dns for  fn_out 00006721\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 4. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 1055 extension lines and 1268 extension nodes\n",
      "marking 2441 nodes as conditioning\n",
      "making nodes lookup: 57.8\n",
      "get output children: 2.138\n",
      "mark dist from end: 0.016\n",
      "mark dist from start: 2.363\n",
      "denote module children: 3.396\n",
      "nestify first stuff: 0.076\n",
      "x pos: 0.442\n",
      "y pos: 86.278\n",
      "marking coords: 154.182\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import (\n",
    "    StableCascadeDecoderPipeline,\n",
    "    StableCascadePriorPipeline,\n",
    "    StableCascadeUNet,\n",
    ")\n",
    "prompt = \"an image of a shiba inu, donning a spacesuit and helmet\"\n",
    "negative_prompt = \"\"\n",
    "\n",
    "prior_unet = StableCascadeUNet.from_pretrained(\"stabilityai/stable-cascade-prior\", subfolder=\"prior_lite\")\n",
    "prior = StableCascadePriorPipeline.from_pretrained(\"stabilityai/stable-cascade-prior\", prior=prior_unet)\n",
    "prior.enable_model_cpu_offload()\n",
    "\n",
    "with Recorder():\n",
    "    prior_output = prior(\n",
    "        prompt=prompt,\n",
    "        height=512, #1024,\n",
    "        width=512, #1024,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=4.0,\n",
    "        num_images_per_prompt=1,\n",
    "        num_inference_steps=1 #20\n",
    "    )\n",
    "\n",
    "mn = \"stable-cascade-prior\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...:  40%|████      | 2/5 [00:00<00:00,  4.99it/s]"
     ]
    }
   ],
   "source": [
    "decoder_unet = StableCascadeUNet.from_pretrained(\"stabilityai/stable-cascade\", subfolder=\"decoder_lite\")\n",
    "decoder = StableCascadeDecoderPipeline.from_pretrained(\"stabilityai/stable-cascade\", decoder=decoder_unet)\n",
    "# decoder.enable_model_cpu_offload()\n",
    "image_embeddings = torch.randn([1, 16, 12, 12])\n",
    "\n",
    "with Recorder():\n",
    "    decoder_output = decoder(\n",
    "        image_embeddings=image_embeddings,\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        guidance_scale=0.0,\n",
    "        output_type=\"pil\",\n",
    "        num_inference_steps=1, #10\n",
    "    ).images[0]\n",
    "    \n",
    "mn = \"stable-cascade-decoder\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  4.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# load both base & refiner\n",
    "base = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-base-1.0\", \n",
    "    # torch_dtype=torch.float16, \n",
    "    # variant=\"fp16\", \n",
    "    # use_safetensors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 4130 nodes and 188 modules\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n global output nodes 49\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 6\n",
      "no dns for  mod_out 00004219\n",
      "no dns for  fn_out 00004317\n",
      "no dns for  mod_out 00004221\n",
      "no dns for  mod_out 00004223\n",
      "no dns for  mod_out 00004225\n",
      "no dns for  mod_out 00004227\n",
      "no dns for  mod_out 00004229\n",
      "no dns for  mod_out 00004231\n",
      "no dns for  mod_out 00004233\n",
      "no dns for  mod_out 00004235\n",
      "no dns for  mod_out 00004237\n",
      "no dns for  mod_out 00004239\n",
      "no dns for  mod_out 00004241\n",
      "no dns for  mod_out 00004243\n",
      "no dns for  mod_out 00004245\n",
      "no dns for  mod_out 00004247\n",
      "no dns for  mod_out 00004249\n",
      "no dns for  mod_out 00004251\n",
      "no dns for  mod_out 00004253\n",
      "no dns for  mod_out 00004255\n",
      "no dns for  mod_out 00004257\n",
      "no dns for  mod_out 00004259\n",
      "no dns for  mod_out 00004261\n",
      "no dns for  mod_out 00004263\n",
      "no dns for  mod_out 00004265\n",
      "no dns for  mod_out 00004267\n",
      "no dns for  mod_out 00004269\n",
      "no dns for  mod_out 00004271\n",
      "no dns for  mod_out 00004273\n",
      "no dns for  mod_out 00004275\n",
      "no dns for  mod_out 00004277\n",
      "no dns for  mod_out 00004279\n",
      "no dns for  mod_out 00004283\n",
      "no dns for  mod_out 00004217\n",
      "no dns for  fn_out 00004315\n",
      "no dns for  mod_out 00001202\n",
      "no dns for  mod_out 00001204\n",
      "no dns for  mod_out 00001206\n",
      "no dns for  mod_out 00001208\n",
      "no dns for  mod_out 00001210\n",
      "no dns for  mod_out 00001212\n",
      "no dns for  mod_out 00001214\n",
      "no dns for  mod_out 00001216\n",
      "no dns for  mod_out 00001218\n",
      "no dns for  mod_out 00001220\n",
      "no dns for  mod_out 00001224\n",
      "no dns for  mod_out 00001196\n",
      "no dns for  mod_out 00001198\n",
      "no dns for  mod_out 00001200\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 4. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 366 extension lines and 406 extension nodes\n",
      "marking 234 nodes as conditioning\n",
      "making nodes lookup: 4.406\n",
      "get output children: 0.216\n",
      "mark dist from end: 0.005\n",
      "mark dist from start: 0.227\n",
      "denote module children: 0.185\n",
      "nestify first stuff: 0.03\n",
      "x pos: 0.115\n",
      "y pos: 10.608\n",
      "marking coords: 16.461\n"
     ]
    }
   ],
   "source": [
    "n_steps = 1 #40\n",
    "high_noise_frac = 0.8 # what % of steps to be run on each experts (80/20) here\n",
    "\n",
    "prompt = \"A majestic lion jumping from a big stone at night\"\n",
    "\n",
    "with Recorder():\n",
    "\n",
    "    image = base(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=n_steps,\n",
    "        denoising_end=high_noise_frac,\n",
    "        output_type=\"latent\",\n",
    "    ).images\n",
    "\n",
    "mn = \"sd-xl-base\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 5/5 [00:01<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n",
      "got duplicate tensors in output bundle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [03:56<00:00, 236.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import torch\n",
    "\n",
    "refiner = DiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-xl-refiner-1.0\",\n",
    "    text_encoder_2=base.text_encoder_2,\n",
    "    vae=base.vae,\n",
    "    # torch_dtype=torch.float16,\n",
    "    # use_safetensors=True,\n",
    "    # variant=\"fp16\",\n",
    ")\n",
    "\n",
    "image = torch.randn(1,4,128,128)\n",
    "with Recorder():\n",
    "    image = refiner(\n",
    "        prompt=prompt,\n",
    "        num_inference_steps=n_steps,\n",
    "        denoising_start=high_noise_frac,\n",
    "        image=image,\n",
    "    ).images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 12142 nodes and 565 modules\n",
      "no execution counter for global output node  numpy\n",
      "n global output nodes 67\n",
      "There are 0 nodes left unmarked for 'dist to end'\n",
      "num global input nodes 9\n",
      "no dns for  numpy 00012706\n",
      "no dns for  mod_out 00002994\n",
      "no dns for  mod_out 00002996\n",
      "no dns for  mod_out 00002998\n",
      "no dns for  mod_out 00003000\n",
      "no dns for  mod_out 00003002\n",
      "no dns for  mod_out 00003004\n",
      "no dns for  mod_out 00003006\n",
      "no dns for  mod_out 00003008\n",
      "no dns for  mod_out 00003010\n",
      "no dns for  mod_out 00003012\n",
      "no dns for  mod_out 00003014\n",
      "no dns for  mod_out 00003016\n",
      "no dns for  mod_out 00003018\n",
      "no dns for  mod_out 00003020\n",
      "no dns for  mod_out 00003022\n",
      "no dns for  mod_out 00003024\n",
      "no dns for  mod_out 00003026\n",
      "no dns for  mod_out 00003028\n",
      "no dns for  mod_out 00003030\n",
      "no dns for  mod_out 00003032\n",
      "no dns for  mod_out 00003034\n",
      "no dns for  mod_out 00003036\n",
      "no dns for  mod_out 00003038\n",
      "no dns for  mod_out 00003040\n",
      "no dns for  mod_out 00003042\n",
      "no dns for  mod_out 00003044\n",
      "no dns for  mod_out 00003046\n",
      "no dns for  mod_out 00003048\n",
      "no dns for  mod_out 00003050\n",
      "no dns for  mod_out 00003052\n",
      "no dns for  mod_out 00003054\n",
      "no dns for  mod_out 00003058\n",
      "no dns for  mod_out 00002992\n",
      "no dns for  mod_out 00006055\n",
      "no dns for  mod_out 00006057\n",
      "no dns for  mod_out 00006059\n",
      "no dns for  mod_out 00006061\n",
      "no dns for  mod_out 00006063\n",
      "no dns for  mod_out 00006065\n",
      "no dns for  mod_out 00006067\n",
      "no dns for  mod_out 00006069\n",
      "no dns for  mod_out 00006071\n",
      "no dns for  mod_out 00006073\n",
      "no dns for  mod_out 00006075\n",
      "no dns for  mod_out 00006077\n",
      "no dns for  mod_out 00006079\n",
      "no dns for  mod_out 00006081\n",
      "no dns for  mod_out 00006083\n",
      "no dns for  mod_out 00006085\n",
      "no dns for  mod_out 00006087\n",
      "no dns for  mod_out 00006089\n",
      "no dns for  mod_out 00006091\n",
      "no dns for  mod_out 00006093\n",
      "no dns for  mod_out 00006095\n",
      "no dns for  mod_out 00006097\n",
      "no dns for  mod_out 00006099\n",
      "no dns for  mod_out 00006101\n",
      "no dns for  mod_out 00006103\n",
      "no dns for  mod_out 00006105\n",
      "no dns for  mod_out 00006107\n",
      "no dns for  mod_out 00006109\n",
      "no dns for  mod_out 00006111\n",
      "no dns for  mod_out 00006113\n",
      "no dns for  mod_out 00006115\n",
      "no dns for  mod_out 00006119\n",
      "no dns for  mod_out 00006053\n",
      "There are 0 nodes left unmarked for 'dist to start'\n",
      "Module inputs: 4. Module outputs: 0\n",
      "n unmarked x_relative 0\n",
      "added 851 extension lines and 970 extension nodes\n",
      "marking 1458 nodes as conditioning\n",
      "making nodes lookup: 51.578\n",
      "get output children: 1.932\n",
      "mark dist from end: 0.021\n",
      "mark dist from start: 1.948\n",
      "denote module children: 2.512\n",
      "nestify first stuff: 0.077\n",
      "x pos: 0.291\n",
      "y pos: 54.576\n",
      "marking coords: 114.857\n"
     ]
    }
   ],
   "source": [
    "mn = \"sd-xl-refiner\"\n",
    "model_coords_marker = ModelCoordsMarker(gc.nodes, gc.modules, mn)\n",
    "model_coords_marker.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
